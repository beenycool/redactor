{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e152e12",
   "metadata": {},
   "source": [
    "# Qwen PII Double-Check Service (Colab)\n",
    "\n",
    "This notebook launches a lightweight Flask (or FastAPI) service that exposes a `/health` and `/check_pii` endpoint, then tunnels it via ngrok so the frontend can call it.\n",
    "\n",
    "Instructions:\n",
    "1. Run cells top to bottom.\n",
    "2. Copy the public ngrok URL printed in the last cell and paste it into the frontend Qwen URL field.\n",
    "3. Ensure the URL starts with `https://` and do *not* include a trailing slash.\n",
    "\n",
    "Security note: This demo tunnel is public; avoid sending real sensitive data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f3ec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (quiet where possible)\n",
    "!pip -q install flask pyngrok transformers accelerate torch --upgrade\n",
    "\n",
    "import subprocess, sys\n",
    "print('Versions:')\n",
    "for pkg in ['flask','pyngrok','transformers','torch','accelerate']:\n",
    "    try:\n",
    "        mod = __import__(pkg)\n",
    "        print(f\"  {pkg}: {getattr(mod, '__version__', 'n/a')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  {pkg}: error {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f699258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, re, math, time, threading, traceback, uuid\n",
    "from typing import List, Dict, Any\n",
    "from flask import Flask, request, jsonify\n",
    "from pyngrok import ngrok\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen3-4B-Thinking-2507\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "print(f\"Loading model {MODEL_NAME} on {DEVICE} ...\")\n",
    "start_load = time.time()\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=DTYPE,\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "    )\n",
    "    if not torch.cuda.is_available():\n",
    "        model.to(DEVICE)\n",
    "except Exception as e:\n",
    "    print(\"Failed to load model:\", e)\n",
    "    raise\n",
    "print(f\"Model loaded in {time.time()-start_load:.2f}s\")\n",
    "\n",
    "# Simple regex fallbacks (used after model suggestions to ensure spans)\n",
    "FALLBACK_PATTERNS = {\n",
    "    'EMAIL': re.compile(r'[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}'),\n",
    "    'PHONE': re.compile(r'\\b(?:\\+?1[-.\\s]?)?(?:\\(\\d{3}\\)|\\d{3})[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b'),\n",
    "    'SSN': re.compile(r'\\b\\d{3}-\\d{2}-\\d{4}\\b'),\n",
    "    'IP_ADDRESS': re.compile(r'\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b'),\n",
    "    'URL': re.compile(r'https?://[\\w./%-]+'),\n",
    "}\n",
    "\n",
    "SYSTEM_INSTRUCTION = (\n",
    "    \"You are a precise privacy auditor. Given ORIGINAL_TEXT and a list of EXISTING_REDACTIONS (with original values removed already), \"\n",
    "    \"identify any REMAINING personally identifiable information (PII) or sensitive data that still appears. \"\n",
    "    \"Return ONLY strict JSON with the shape: {\\n\"\n",
    "    \"  \\\"additional_redactions\\\": [ {\\n\"\n",
    "    \"     \\\"value\\\": <exact substring>,\\n\"\n",
    "    \"     \\\"type\\\": <UPPER_SNAKE_TYPE>,\\n\"\n",
    "    \"     \\\"confidence\\\": <0-1 float>\\n\"\n",
    "    \"  }, ... ],\\n\"\n",
    "    \"  \\\"reasoning\\\": <brief string>,\\n\"\n",
    "    \"  \\\"confidence\\\": <overall 0-1 float>\\n\"\n",
    "    \"}\\n\"\n",
    "    \"Rules: (1) value must be an exact substring from ORIGINAL_TEXT. (2) Do not include items already covered by EXISTING_REDACTIONS. \"\n",
    "    \"(3) Types should be short UPPER tokens like EMAIL, PHONE, NAME, ADDRESS, SSN, ACCOUNT_NUMBER, URL, IP_ADDRESS. \"\n",
    "    \"(4) If nothing new, return an empty additional_redactions array. Output ONLY JSON.\"\n",
    ")\n",
    "\n",
    "# Canonical type normalization mapping (light)\n",
    "CANON_MAP = {\n",
    "    'PERSON': 'NAME', 'GIVENNAME': 'NAME', 'SURNAME': 'NAME'\n",
    "}\n",
    "\n",
    "def canonical_type(t: str) -> str:\n",
    "    t = t.upper().strip()\n",
    "    return CANON_MAP.get(t, t)\n",
    "\n",
    "def _extract_json_objects(text: str) -> List[str]:\n",
    "    # Robust JSON object extraction using stack counting\n",
    "    objs = []\n",
    "    stack = 0\n",
    "    start_idx = None\n",
    "    for i, ch in enumerate(text):\n",
    "        if ch == '{':\n",
    "            if stack == 0:\n",
    "                start_idx = i\n",
    "            stack += 1\n",
    "        elif ch == '}':\n",
    "            if stack > 0:\n",
    "                stack -= 1\n",
    "                if stack == 0 and start_idx is not None:\n",
    "                    candidate = text[start_idx:i+1]\n",
    "                    objs.append(candidate)\n",
    "                    start_idx = None\n",
    "    return objs\n",
    "\n",
def generate_model_json(prompt: str, max_new_tokens: int = 768) -> Dict[str, Any]:
    enc = tokenizer(prompt, return_tensors="pt").to(model.device)
    input_len = enc["input_ids"].shape[1]
    with torch.no_grad():
        output = model.generate(
            **enc,
            max_new_tokens=max_new_tokens,
            temperature=0.2,
            do_sample=False,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,
        )
    # Decode only the newly generated tokens
    gen_ids = output[0][input_len:]
    decoded = tokenizer.decode(gen_ids, skip_special_tokens=True)

    # Try strict extraction first
    for obj_str in reversed(_extract_json_objects(decoded)):
        try:
            return json.loads(obj_str)
        except Exception:
            continue
    # Fallback: attempt to clean code fences and retry
    cleaned = re.sub(r'
    "\n",
    "def build_prompt(text: str, existing: List[Dict[str, Any]]) -> str:\n",
    "    existing_values = [e.get('value') for e in existing if e.get('value')]\n",
    "    existing_section = json.dumps(existing_values[:200], ensure_ascii=False, indent=2)\n",
    "    return f\"<|system|>\\n{SYSTEM_INSTRUCTION}\\n<|user|>\\nORIGINAL_TEXT:\\n{text}\\n\\nEXISTING_REDACTIONS (values only):\\n{existing_section}\\n\\nRespond with JSON only.\\n<|assistant|>\"\n",
    "\n",
    "# Span utilities\n",
    "\n",
    "def spans_overlap(a_start, a_end, b_start, b_end):\n",
    "    return a_start < b_end and a_end > b_start\n",
    "\n",
    "\n",
    "def build_existing_spans(existing: List[Dict[str, Any]]):\n",
    "    return [(e['start'], e['end']) for e in existing if 'start' in e and 'end' in e]\n",
    "\n",
    "\n",
    "def first_non_overlapping_occurrence(text: str, value: str, occupied: List[tuple]):\n",
    "    start = 0\n",
    "    while True:\n",
    "        idx = text.find(value, start)\n",
    "        if idx == -1:\n",
    "            return None\n",
    "        end = idx + len(value)\n",
    "        if not any(spans_overlap(idx, end, s, e) for s, e in occupied):\n",
    "            return (idx, end)\n",
    "        start = idx + 1\n",
    "\n",
    "\n",
    "def enrich_model_additions(text: str, existing: List[Dict[str, Any]], model_obj: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    existing_spans = build_existing_spans(existing)\n",
    "    additions = []\n",
    "    for item in model_obj.get('additional_redactions', []) or []:\n",
    "        raw_val = item.get('value')\n",
    "        raw_type = canonical_type(item.get('type', 'MISC'))\n",
    "        if not raw_val or len(raw_val) > len(text):\n",
    "            continue\n",
    "        occ = first_non_overlapping_occurrence(text, raw_val, existing_spans)\n",
    "        if occ:\n",
    "            s, e = occ\n",
    "            existing_spans.append( (s,e) )\n",
    "            additions.append({\n",
    "                'value': raw_val,\n",
    "                'type': raw_type,\n",
    "                'start': s,\n",
    "                'end': e,\n",
    "                'confidence': float(item.get('confidence', 0.75))\n",
    "            })\n",
    "    # Fallback regex sweep for obvious missed patterns\n",
    "    for ptype, pat in FALLBACK_PATTERNS.items():\n",
    "        for m in pat.finditer(text):\n",
    "            s,e = m.start(), m.end()\n",
    "            if any(spans_overlap(s,e,es,ee) for es,ee in existing_spans):\n",
    "                continue\n",
    "            val = m.group(0)\n",
    "            existing_spans.append((s,e))\n",
    "            additions.append({\n",
    "                'value': val,\n",
    "                'type': ptype,\n",
    "                'start': s,\n",
    "                'end': e,\n",
    "                'confidence': 0.70\n",
    "            })\n",
    "    reasoning = model_obj.get('reasoning', 'Model + regex secondary scan applied.')\n",
    "    overall_conf = float(model_obj.get('confidence', 0.80 if additions else 0.50))\n",
    "    return {\n",
    "        'additional_redactions': additions,\n",
    "        'reasoning': reasoning,\n",
    "        'confidence': overall_conf\n",
    "    }\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.get('/health')\n",
    "def health():\n",
    "    return {'status': 'ok', 'model': MODEL_NAME}\n",
    "\n",
    "@app.post('/check_pii')\n",
    "def check_pii():\n",
    "    try:\n",
    "        data = request.get_json(force=True) or {}\n",
    "        text = data.get('text','')\n",
    "        existing = data.get('existing_redactions') or []\n",
    "        if not isinstance(existing, list):\n",
    "            existing = []\n",
    "        prompt = build_prompt(text, existing)\n",
    "        raw_obj = generate_model_json(prompt)\n",
    "        enriched = enrich_model_additions(text, existing, raw_obj)\n",
    "        return jsonify(enriched)\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        return jsonify({\n",
    "            'additional_redactions': [],\n",
    "            'reasoning': f'Error: {e}',\n",
    "            'confidence': 0.0\n",
    "        }), 500\n",
    "\n",
    "# --- Launch server + ngrok tunnel ---\n",
    "PORT = 5000\n",
    "\n",
    "# Configure ngrok auth token if provided\n",
    "NGROK_TOKEN = os.environ.get('NGROK_AUTH_TOKEN') or os.environ.get('NGROK_TOKEN')\n",
    "if NGROK_TOKEN:\n",
    "    try:\n",
    "        ngrok.set_auth_token(NGROK_TOKEN)\n",
    "        print('Ngrok auth token set.')\n",
    "    except Exception as e:\n",
    "        print('Failed to set ngrok auth token:', e)\n",
    "else:\n",
    "    print('No ngrok auth token found in environment (NGROK_AUTH_TOKEN). You may hit connection limits.')\n",
    "\n",
    "def run_app():\n",
    "    app.run(host='0.0.0.0', port=PORT)\n",
    "\n",
    "threading.Thread(target=run_app, daemon=True).start()\n",
    "public_url = ngrok.connect(PORT, bind_tls=True).public_url\n",
    "print('\\nPublic Qwen double-check URL:', public_url)\n",
    "print('Health endpoint:', public_url + '/health')\n",
    "print('Example check_pii endpoint:', public_url + '/check_pii')\n",
    "print('\\nPaste the base URL (without trailing slash) into the frontend Qwen field.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93404567",
   "metadata": {},
   "source": [
    "Copy the 'Public Qwen double-check URL' (without trailing slash) into the frontend UI field to enable Qwen Double-Check."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344e10da",
   "metadata": {},
   "source": [
    "### Usage Notes\n",
    "1. (Optional) Set your ngrok token in the Colab session before running: `os.environ['NGROK_AUTH_TOKEN'] = 'xxxx'` or use the Colab UI (Secrets).\n",
    "2. Run the install cell, then the model server cell.\n",
    "3. Wait for the public URL to appear. Copy it (no trailing slash) into the frontend Qwen URL field and click Connect.\n",
    "4. The frontend will call `/health` and then use `/check_pii` for secondary PII detection.\n",
    "5. If you see 500 errors, expand the Colab cell output to view traceback; often JSON formatting errors indicate the model responded with non-JSONâ€”rerun the second cell.\n",
    "6. To reduce cost/latency, you can switch to a smaller model by changing `MODEL_NAME` in the second cell.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
